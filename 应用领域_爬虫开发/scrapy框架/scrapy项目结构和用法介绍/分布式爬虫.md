# 介绍

分布式爬虫从名字就可以看出是分布式的一种爬虫，通过特定手段让爬虫分别部署在不同的位置然后用其调度器来调度分布在不同位置的爬虫进行数据采集，
保证了请求的不重复，如果在`settings.py`文件中设置了`SCHEDULER_PERSIST = True`完成任务后不会自动退出，它会一直连接 Redis，等待你下次再推送新的URL,
但是默认是为`SCHEDULER_PERSIST = False`/没有，需要手动添加

经验：去重是传入网址后里面获取的url被请求后才会去重，如果你lpush/rpush相同的一个网址是不会被记录去重的，因为你手动push的请求不是ScrapyRequest对象序列化的格式
对应方案：如果一个页面有多一个需要请求的类似网址直接push该页面的就行，不要傻乎乎的分开push，这样不会被去重

# 使用

scrapy-redis 不是官方 Scrapy 团队维护的库，而是由社区开发者封装的第三方扩展，用于将 Scrapy 与 Redis 集成，实现分布式爬虫。

源码：https://github.com/rmax/scrapy-redis     
按照里面的模仿即可`example-project/example`

- 编写分布式爬虫步骤：
    1. 编写修改继承spider文件
    2. 添加scrapy-redis扩展属性到settings.py文件中
    3. 启动爬虫实例，启动方法都是和规则/普通爬虫一样的命令，启动完成后会一直获取redis的key看是否有任务需要处理
    4. push到redis中，默认使用的是第0个数据库(可自行定义)
    5. 等待数据爬取即可，如果开启了保存到redis的item就会在redis中找到爬取的数据(可选的)

注意：保存的数据如果有中文会变成Unicode编码，但是这是正常现象，如果你执意需要更改只能更改scrapy-redis的管道源码，但是这是不被建议的

- 经验！！！！--运行逻辑场景：
  - scrapy爬虫怎么编写的就怎么编写分布式爬虫，不需要你先获取所有链接然后一一推送到redis，你可以在项目中写这部分代码。
  - scrapy-redis会自动处理你获取到的一群链接自动处理为他们对应的调度类型，然后分别调度执行。
  - 实例都包含相同代码，但并不是都执行； 
  - 只有抢到某个 URL 的实例才会执行对应的 callback； 
  - 任务在多个实例间是自动、动态分发的； 
  - 你不需要自己控制哪个实例处理哪一类页面，Scrapy-Redis 会帮你调度。

# 主要替换板块

## settings.py

添加下下面的变量属性到设置文件即可。ITEM_PIPELINES、DOWNLOAD_DELAY可能编写爬虫时已经设置过了，需要注意追加/更改

```python
"""
使用 scrapy-redis 提供的调度器和去重类
断点续爬：当爬虫停止时，队列中的 URL 以及已爬取的 URL 会保留在 Redis 中，爬虫重启时可以接着未爬取的部分继续抓取，避免重新抓取已处理过的 URL。
"""
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"  # 过滤器
SCHEDULER = "scrapy_redis.scheduler.Scheduler"  # 调度器

# 可选（推荐开启）：是否允许暂停后恢复（持久化队列）
SCHEDULER_PERSIST = True

# Redis 连接配置（默认本地）
REDIS_HOST = 'localhost'
REDIS_PORT = 6379

# 可选：设置 redis 密码
# REDIS_PARAMS = {
#     'password': 'yourpassword',
#     'db': 0  # 可选，默认是 0
# }

# 可选：使用优先级队列（默认是 FIFO）
# SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'
# SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue'
# SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue'

# 可选：配置 item pipeline，把爬取结果写入 redis
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 300,
}

# 可选：Redis item 存储键名（默认是 spider 名）
# REDIS_ITEMS_KEY = '%(spider)s:items'

# 下载速率限制 /s ,不然会导致一下请求过高导致对方服务器相应不过来
DOWNLOAD_DELAY = 1
```

## spider/*文件

更改继承类，RedisSpider是普通爬虫，RedisCrawlSpider是规则爬虫，需要哪个就继承哪个即可

分布式爬虫只是在scrapy爬虫扩展的一部分，完全可以使用scrapy爬虫的属性等

```python
import scrapy
from scrapy_redis.spiders import RedisSpider
from ..items import Project5Item


class MySpider(RedisSpider):
    name = 'my_spider'
    redis_key = 'my_spider:start_urls'  # Redis 中的起始请求列表键名，仿照写即可

    def parse(self, response):
        """正常下一页网址提取"""
        links = response.css('.quote > span > a::attr(href)').getall()

        """使用回调函数执行再一次数据请求"""
        for link in links:
            yield scrapy.Request(url='http://quotes.toscrape.com' + link, callback=self.parse1)

    def parse1(self, response):
        """处理数据清洗等操作"""
        born_date = response.css('.author-born-date::text').get()
        born_location = response.css('.author-born-location::text').get()

        item = Project5Item(born_location=born_location, born_date=born_date)
        yield item
```

## redis push

```redis
-- 如果还需要传入headers，cookies等都可以这样，默认回调函数是parse可以选择不写
lpush my_spider:start_urls '{"url": "http://example.com", "callback": "parse"}'
```

