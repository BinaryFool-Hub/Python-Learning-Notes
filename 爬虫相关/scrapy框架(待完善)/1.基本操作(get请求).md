# 爬虫文件的 start_urls 列表

在列表里面的都会进行请求，返回给 parse() 函数 的 response

# 爬虫文件 parse() 函数

## 参数 response

response = request<请求体> + response<响应体> + parsel<css, xpath, re>

可以进行数据提取等操作

## yield返回（因为是异步）

- 返回方式一
  **这种方法可以不使用items的class，不推荐**

```
yield {
    'text': text
}
```

- 返回方式二
  **在 items 文件定义了数据结构，推荐**

```
yield ProjectnameItem(text=text)
```

# 管道文件

处理的数据都会流入管道文件，进行数据的保存和其他操作

需要使用管道文件就需要开启设置，在 settings 文件的 ITEM_PIPELINES 变量中，取消注释即可开启管道，
管道路径的右侧数字是权重，数值越小，优先级越高。Scrapy 会按照优先级从低到高（即数值从小到大）依次调用管道。



