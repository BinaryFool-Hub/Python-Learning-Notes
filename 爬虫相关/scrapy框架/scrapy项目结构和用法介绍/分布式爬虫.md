# 介绍

分布式爬虫从名字就可以看出是分布式的一种爬虫，通过特定手段让爬虫分别部署在不同的位置然后用其调度器来调度分布在不同位置的爬虫进行数据采集，
保证了请求的不重复，如果在`settings.py`文件中设置了`SCHEDULER_PERSIST = True`完成任务后不会自动退出，它会一直连接 Redis，等待你下次再推送新的URL,
但是默认是为`SCHEDULER_PERSIST = False`/没有，需要手动添加

经验：去重是传入网址后里面获取的url被请求后才会去重，如果你lpush/rpush相同的一个网址是不会被记录去重的，因为你手动push的请求不是ScrapyRequest对象序列化的格式
对应方案：如果一个页面有多一个需要请求的类似网址直接push该页面的就行，不要傻乎乎的分开push，这样不会被去重

# 使用

scrapy-redis 不是官方 Scrapy 团队维护的库，而是由社区开发者封装的第三方扩展，用于将 Scrapy 与 Redis 集成，实现分布式爬虫。

源码：https://github.com/rmax/scrapy-redis

按照里面的模仿即可`example-project/example`

- 编写步骤：
    1. 编写修改继承spider文件
    2. 添加scrapy-redis扩展属性到settings.py文件中
    3. push到redis中，默认使用的是第0个数据库(可自行定义)
    4. 等待数据爬取即可，如果开启了保存到redis的item就会在redis中找到爬取的数据(可选的)

# 主要替换板块

## settings.py

添加下下面的变量属性到设置文件即可。ITEM_PIPELINES、DOWNLOAD_DELAY可能编写爬虫时已经设置过了，需要注意追加/更改

```python
"""
使用 scrapy-redis 提供的调度器和去重类
断点续爬：当爬虫停止时，队列中的 URL 以及已爬取的 URL 会保留在 Redis 中，爬虫重启时可以接着未爬取的部分继续抓取，避免重新抓取已处理过的 URL。
"""
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"  # 过滤器
SCHEDULER = "scrapy_redis.scheduler.Scheduler"  # 调度器

# 可选（推荐开启）：是否允许暂停后恢复（持久化队列）
SCHEDULER_PERSIST = True

# Redis 连接配置（默认本地）
REDIS_HOST = 'localhost'
REDIS_PORT = 6379

# 可选：设置 redis 密码
# REDIS_PARAMS = {
#     'password': 'yourpassword',
#     'db': 0  # 可选，默认是 0
# }

# 可选：使用优先级队列（默认是 FIFO）
# SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'
# SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue'
# SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue'

# 可选：配置 item pipeline，把爬取结果写入 redis
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 300,
}

# 可选：Redis item 存储键名（默认是 spider 名）
# REDIS_ITEMS_KEY = '%(spider)s:items'

# 下载速率限制 /s ,不然会导致一下请求过高导致对方服务器相应不过来
DOWNLOAD_DELAY = 1
```

## spider/*文件

更改继承类，RedisSpider是普通爬虫，RedisCrawlSpider是规则爬虫，需要哪个就继承哪个即可

分布式爬虫只是在scrapy爬虫扩展的一部分，完全可以使用scrapy爬虫的属性等

```python
import scrapy
from scrapy_redis.spiders import RedisSpider


class MySpider(RedisSpider):
    name = 'my_spider'
    redis_key = 'my_spider:start_urls'  # Redis 中的起始请求列表键名，仿照写即可

    def parse(self, response):
        """正常下一页网址提取"""
        links = response.css('.quote > span > a::attr(href)').getall()

        """使用回调函数执行再一次数据请求"""
        for link in links:
            yield scrapy.Request(url='http://quotes.toscrape.com' + link, callback=self.parse1)

    def parse1(self, response):
        """处理数据清洗等操作"""
        born_date = response.css('.author-born-date::text').get()
        born_location = response.css('.author-born-location::text').get()

        item = Project5Item(born_location=born_location, born_date=born_date)
        yield item
```

## redis push

```redis
lpush my_spider:start_urls "http://example.com"
```

